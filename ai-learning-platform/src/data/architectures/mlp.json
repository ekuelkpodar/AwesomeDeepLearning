{
  "id": "mlp",
  "name": "Multi-Layer Perceptron (MLP)",
  "category": "feedforward",
  "subcategory": "basic",
  "year": 1986,
  "authors": ["David Rumelhart", "Geoffrey Hinton", "Ronald Williams"],
  "paper": "Learning representations by back-propagating errors",
  "paperUrl": "https://www.nature.com/articles/323533a0",
  "description": "The Multi-Layer Perceptron (MLP) is a feedforward neural network with one or more hidden layers between input and output. By adding hidden layers and using backpropagation for training, MLPs can learn complex non-linear patterns that single-layer perceptrons cannot.",
  "plainEnglish": "Think of the MLP as a team of decision-makers working in layers. The first layer looks at raw data, the middle layers (hidden layers) find increasingly complex patterns, and the final layer makes the decision. For example, in image recognition: Layer 1 finds edges, Layer 2 combines edges into shapes, Layer 3 recognizes objects. Each neuron is like a specialist that becomes an expert at detecting one specific pattern.",
  "keyInnovation": "Introduced hidden layers with non-linear activation functions and backpropagation algorithm, enabling neural networks to learn complex, non-linear relationships. This solved the XOR problem and launched the modern deep learning era.",
  "architecture": {
    "layers": [
      {
        "type": "input",
        "name": "Input Layer",
        "params": {"units": 784},
        "outputShape": [784],
        "description": "Accepts input features (e.g., 28×28 flattened image)"
      },
      {
        "type": "fc",
        "name": "Hidden Layer 1",
        "params": {"units": 128, "activation": "relu"},
        "outputShape": [128],
        "description": "First hidden layer learns low-level features"
      },
      {
        "type": "fc",
        "name": "Hidden Layer 2",
        "params": {"units": 64, "activation": "relu"},
        "outputShape": [64],
        "description": "Second hidden layer learns higher-level features"
      },
      {
        "type": "fc",
        "name": "Output Layer",
        "params": {"units": 10, "activation": "softmax"},
        "outputShape": [10],
        "description": "Classification layer (10 classes)"
      }
    ],
    "parameters": 109386,
    "depth": 3,
    "inputShape": [784],
    "outputShape": [10]
  },
  "mathematics": {
    "equations": [
      {
        "name": "Hidden Layer Forward Pass",
        "latex": "h^{(l)} = \\sigma(W^{(l)}h^{(l-1)} + b^{(l)})",
        "description": "Each hidden layer applies weights, adds bias, and passes through activation function"
      },
      {
        "name": "ReLU Activation",
        "latex": "\\text{ReLU}(x) = \\max(0, x)",
        "description": "Most common activation for hidden layers - outputs input if positive, else 0"
      },
      {
        "name": "Softmax Output",
        "latex": "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}",
        "description": "Converts output logits to probability distribution over K classes"
      },
      {
        "name": "Cross-Entropy Loss",
        "latex": "L = -\\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)",
        "description": "Measures difference between predicted and true probability distributions"
      },
      {
        "name": "Backpropagation (Chain Rule)",
        "latex": "\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial h^{(l+1)}} \\frac{\\partial h^{(l+1)}}{\\partial W^{(l)}}",
        "description": "Gradients propagate backwards through layers using the chain rule"
      },
      {
        "name": "Weight Update (SGD)",
        "latex": "W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}",
        "description": "Update weights in direction that reduces loss"
      }
    ],
    "forwardPass": [
      "Flatten input (if needed): x ∈ ℝⁿ",
      "First hidden layer: h₁ = ReLU(W₁x + b₁)",
      "Second hidden layer: h₂ = ReLU(W₂h₁ + b₂)",
      "Output layer: ŷ = softmax(W₃h₂ + b₃)",
      "Return probability distribution over classes"
    ],
    "backpropagation": [
      "Compute output layer gradient: δ₃ = ŷ - y",
      "Backpropagate to hidden layer 2: δ₂ = (W₃ᵀδ₃) ⊙ ReLU'(h₂)",
      "Backpropagate to hidden layer 1: δ₁ = (W₂ᵀδ₂) ⊙ ReLU'(h₁)",
      "Compute weight gradients: ∂L/∂Wₗ = δₗhₗ₋₁ᵀ",
      "Update all weights and biases using gradient descent"
    ],
    "lossFunction": "Cross-Entropy for classification, MSE for regression"
  },
  "code": {
    "pytorch": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nclass MLP(nn.Module):\n    \"\"\"\n    Multi-Layer Perceptron (MLP) with customizable hidden layers.\n    \n    This is the workhorse of deep learning - simple but powerful!\n    Can solve non-linear problems that single-layer perceptrons cannot.\n    \n    Args:\n        input_size: Number of input features\n        hidden_sizes: List of hidden layer sizes [128, 64, 32]\n        output_size: Number of output classes\n        dropout: Dropout probability (0 = no dropout)\n    \"\"\"\n    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.2):\n        super(MLP, self).__init__()\n        \n        # Build layers dynamically\n        layers = []\n        prev_size = input_size\n        \n        # Add hidden layers\n        for hidden_size in hidden_sizes:\n            layers.append(nn.Linear(prev_size, hidden_size))\n            layers.append(nn.ReLU())\n            if dropout > 0:\n                layers.append(nn.Dropout(dropout))\n            prev_size = hidden_size\n        \n        # Add output layer\n        layers.append(nn.Linear(prev_size, output_size))\n        \n        # Combine into sequential model\n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass through the MLP.\n        \n        Args:\n            x: Input tensor (batch_size, input_size)\n            \n        Returns:\n            Output logits (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Example: Training on MNIST-like data\ndef train_mlp_mnist():\n    \"\"\"\n    Train MLP on MNIST digit classification (0-9).\n    This is a classic benchmark for MLPs.\n    \"\"\"\n    # Hyperparameters\n    input_size = 784  # 28x28 flattened image\n    hidden_sizes = [128, 64]  # Two hidden layers\n    output_size = 10  # 10 digit classes\n    batch_size = 128\n    learning_rate = 0.001\n    epochs = 10\n    \n    # Create model\n    model = MLP(input_size, hidden_sizes, output_size, dropout=0.2)\n    print(f'Model created with {sum(p.numel() for p in model.parameters()):,} parameters')\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Dummy data (replace with real MNIST)\n    X_train = torch.randn(1000, input_size)\n    y_train = torch.randint(0, output_size, (1000,))\n    train_dataset = TensorDataset(X_train, y_train)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    # Training loop\n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch_X, batch_y in train_loader:\n            # Forward pass\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            epoch_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += batch_y.size(0)\n            correct += (predicted == batch_y).sum().item()\n        \n        # Print progress\n        accuracy = 100 * correct / total\n        avg_loss = epoch_loss / len(train_loader)\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n    \n    return model\n\n# Alternative: Simple MLP for XOR problem\nclass SimpleXORMLP(nn.Module):\n    \"\"\"\n    Simple 2-layer MLP that can solve XOR problem.\n    This was impossible for single-layer perceptrons!\n    \"\"\"\n    def __init__(self):\n        super(SimpleXORMLP, self).__init__()\n        self.hidden = nn.Linear(2, 4)  # 2 inputs -> 4 hidden neurons\n        self.output = nn.Linear(4, 1)  # 4 hidden -> 1 output\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        x = torch.relu(self.hidden(x))  # Hidden layer with ReLU\n        x = self.sigmoid(self.output(x))  # Output with Sigmoid\n        return x\n\ndef train_xor():\n    \"\"\"Demonstrate MLP solving XOR problem\"\"\"\n    # XOR dataset\n    X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n    y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n    \n    # Create model\n    model = SimpleXORMLP()\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    \n    # Train\n    print('Training MLP on XOR problem...')\n    for epoch in range(5000):\n        # Forward\n        outputs = model(X)\n        loss = criterion(outputs, y)\n        \n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (epoch + 1) % 1000 == 0:\n            print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n    \n    # Test\n    print('\\nXOR Predictions:')\n    with torch.no_grad():\n        for input_val, target in zip(X, y):\n            pred = model(input_val.unsqueeze(0))\n            print(f'Input: {input_val.tolist()}, Target: {int(target)}, Prediction: {pred.item():.4f}')\n\nif __name__ == '__main__':\n    # Demonstrate XOR solution\n    print('=== XOR Problem (impossible for single-layer perceptron) ===')\n    train_xor()\n    \n    print('\\n' + '='*60)\n    print('\\n=== MNIST Classification ===')\n    mnist_model = train_mlp_mnist()",
    "tensorflow": "import tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef create_mlp(input_size, hidden_sizes, output_size, dropout=0.2):\n    \"\"\"\n    Create a Multi-Layer Perceptron using Keras Sequential API.\n    \n    Args:\n        input_size: Number of input features\n        hidden_sizes: List of hidden layer sizes, e.g., [128, 64, 32]\n        output_size: Number of output classes\n        dropout: Dropout rate (0 = no dropout)\n        \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    model = keras.Sequential(name='MLP')\n    \n    # Input layer (implicit in first Dense layer)\n    model.add(layers.InputLayer(input_shape=(input_size,)))\n    \n    # Hidden layers\n    for i, hidden_size in enumerate(hidden_sizes):\n        model.add(layers.Dense(\n            hidden_size, \n            activation='relu',\n            name=f'hidden_{i+1}'\n        ))\n        if dropout > 0:\n            model.add(layers.Dropout(dropout, name=f'dropout_{i+1}'))\n    \n    # Output layer\n    model.add(layers.Dense(\n        output_size, \n        activation='softmax',  # For classification\n        name='output'\n    ))\n    \n    return model\n\n# Example: MNIST classification\ndef train_mlp_mnist():\n    \"\"\"\n    Train MLP on MNIST digit classification.\n    \"\"\"\n    # Load MNIST data\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n    \n    # Preprocess\n    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n    \n    # Create model\n    model = create_mlp(\n        input_size=784,\n        hidden_sizes=[128, 64],\n        output_size=10,\n        dropout=0.2\n    )\n    \n    # Compile\n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Print model summary\n    print('Model Architecture:')\n    model.summary()\n    \n    # Train\n    print('\\nTraining...')\n    history = model.fit(\n        x_train, y_train,\n        batch_size=128,\n        epochs=10,\n        validation_split=0.1,\n        verbose=1\n    )\n    \n    # Evaluate\n    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n    print(f'\\nTest accuracy: {test_acc:.4f}')\n    \n    return model, history\n\n# Alternative: Functional API for more control\ndef create_mlp_functional(input_size, hidden_sizes, output_size):\n    \"\"\"\n    Create MLP using Keras Functional API.\n    Provides more flexibility for complex architectures.\n    \"\"\"\n    # Input\n    inputs = keras.Input(shape=(input_size,), name='input')\n    \n    # Hidden layers\n    x = inputs\n    for i, hidden_size in enumerate(hidden_sizes):\n        x = layers.Dense(hidden_size, activation='relu', name=f'hidden_{i+1}')(x)\n        x = layers.Dropout(0.2, name=f'dropout_{i+1}')(x)\n    \n    # Output\n    outputs = layers.Dense(output_size, activation='softmax', name='output')(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs, name='MLP_Functional')\n    \n    return model\n\n# XOR problem solution\ndef solve_xor_problem():\n    \"\"\"\n    Demonstrate MLP solving XOR - impossible for single-layer perceptron!\n    \"\"\"\n    # XOR dataset\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n    y = np.array([0, 1, 1, 0], dtype=np.float32)\n    \n    # Simple 2-layer MLP\n    model = keras.Sequential([\n        layers.Dense(4, activation='relu', input_shape=(2,)),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    # Compile\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Train\n    print('Solving XOR problem with MLP...')\n    history = model.fit(X, y, epochs=1000, verbose=0)\n    \n    # Test\n    predictions = model.predict(X, verbose=0)\n    print('\\nXOR Results:')\n    for i, (input_val, target, pred) in enumerate(zip(X, y, predictions)):\n        print(f'Input: {input_val}, Target: {int(target)}, Prediction: {pred[0]:.4f}')\n    \n    return model\n\nif __name__ == '__main__':\n    # Solve XOR\n    print('=== XOR Problem Solution ===')\n    xor_model = solve_xor_problem()\n    \n    print('\\n' + '='*60)\n    print('\\n=== MNIST Classification ===')\n    mnist_model, history = train_mlp_mnist()"
  },
  "useCases": [
    {
      "title": "Tabular Data Classification",
      "description": "MLPs excel at structured data with numerical features - the go-to for non-image, non-sequence problems.",
      "examples": [
        "Customer churn prediction from behavioral data",
        "Credit scoring and loan default prediction",
        "Medical diagnosis from patient measurements",
        "Fraud detection in financial transactions"
      ],
      "industry": "Finance, Healthcare, E-commerce"
    },
    {
      "title": "Feature Learning & Embeddings",
      "description": "Hidden layers learn useful representations of data that can be used elsewhere.",
      "examples": [
        "Learning user/item embeddings for recommendation",
        "Dimensionality reduction (autoencoder)",
        "Feature extraction for transfer learning",
        "Anomaly detection through learned representations"
      ],
      "industry": "Recommendation Systems"
    },
    {
      "title": "Function Approximation",
      "description": "MLPs are universal function approximators - they can learn any continuous function given enough neurons.",
      "examples": [
        "Regression tasks (predicting continuous values)",
        "Option pricing in finance",
        "Physics simulations",
        "Control systems and robotics"
      ],
      "industry": "Scientific Computing, Finance"
    }
  ],
  "benchmarks": {
    "datasets": [
      {
        "name": "MNIST",
        "accuracy": 98.4,
        "otherMetrics": {"parameters": "109K", "training_time": "5 min"}
      },
      {
        "name": "Fashion-MNIST",
        "accuracy": 89.2,
        "otherMetrics": {"parameters": "109K"}
      },
      {
        "name": "CIFAR-10 (flattened)",
        "accuracy": 52.1,
        "otherMetrics": {"note": "CNNs much better for images"}
      },
      {
        "name": "XOR Problem",
        "accuracy": 100,
        "otherMetrics": {"epochs": "500", "note": "Single-layer perceptron: 50%"}
      }
    ],
    "performance": {
      "speed": "Very fast - milliseconds per batch on GPU",
      "memory": "Minimal - scales linearly with layer sizes",
      "accuracy": "98%+ on MNIST, competitive on tabular data"
    }
  },
  "trainingTips": {
    "hyperparameters": {
      "learning_rate": "0.001 (Adam) or 0.01 (SGD)",
      "batch_size": "32-256 (depends on dataset size)",
      "hidden_layers": "2-5 layers (more layers = more complex patterns)",
      "hidden_units": "64-512 per layer (larger = more capacity)",
      "dropout": "0.2-0.5 (prevents overfitting)",
      "activation": "ReLU (hidden layers), softmax/sigmoid (output)",
      "optimizer": "Adam (most popular), SGD with momentum",
      "epochs": "10-100 (use early stopping)"
    },
    "commonIssues": [
      {
        "problem": "Vanishing gradients - loss stops decreasing in deep MLPs",
        "solution": "Use ReLU instead of sigmoid/tanh. Consider batch normalization. Keep network shallow (2-5 layers)."
      },
      {
        "problem": "Overfitting - perfect training accuracy, poor test accuracy",
        "solution": "Add dropout (0.2-0.5). Reduce model size. Get more training data. Use early stopping."
      },
      {
        "problem": "Exploding gradients - loss becomes NaN",
        "solution": "Lower learning rate. Use gradient clipping. Check for bugs in data preprocessing."
      },
      {
        "problem": "Model not learning - accuracy stuck at random chance",
        "solution": "Check data normalization (scale to [0,1] or standardize). Verify labels are correct. Try different initialization."
      },
      {
        "problem": "Training too slow",
        "solution": "Increase learning rate (but not too much). Reduce network size. Use batch normalization. Try different optimizer (Adam often fastest)."
      }
    ],
    "dataRequirements": "At least 1000 samples per class. More data = better performance. Works well with 10K-1M samples.",
    "trainingTime": "Minutes to hours depending on size. MNIST: 5 minutes on CPU, 1 minute on GPU."
  },
  "comparisons": ["perceptron", "dnn", "cnn", "logistic-regression"],
  "resources": [
    {
      "type": "paper",
      "title": "Learning representations by back-propagating errors",
      "url": "https://www.nature.com/articles/323533a0",
      "author": "Rumelhart, Hinton, Williams"
    },
    {
      "type": "tutorial",
      "title": "Neural Networks and Deep Learning (free online book)",
      "url": "http://neuralnetworksanddeeplearning.com/",
      "author": "Michael Nielsen"
    },
    {
      "type": "tutorial",
      "title": "Understanding MLPs and Backpropagation",
      "url": "https://www.deeplearningbook.org/contents/mlp.html"
    },
    {
      "type": "blog",
      "title": "Multi-Layer Perceptron Explained",
      "url": "https://towardsdatascience.com/multi-layer-perceptron-explained-with-a-real-life-example-5e29f3b8dfa9"
    }
  ],
  "tags": ["feedforward", "classification", "regression", "fully-connected", "backpropagation", "universal-approximator"],
  "difficulty": "Beginner",
  "computationalRequirements": {
    "minGPU": "None - CPU works fine for small networks",
    "minRAM": "2GB",
    "recommendedGPU": "Any modern GPU (speeds up training 10-50x)",
    "recommendedRAM": "8GB"
  }
}
