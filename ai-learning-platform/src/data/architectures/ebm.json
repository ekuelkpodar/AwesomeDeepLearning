{
  "id": "ebm",
  "name": "Energy-Based Model",
  "category": "energy-based",
  "description": "Models that learn by assigning low energy to data manifold and high energy to other regions, enabling flexible generative modeling without explicit likelihood.",
  "icon": "zap",
  "yearIntroduced": 2006,
  "mathematics": {
    "equations": [
      {
        "name": "Energy Function",
        "latex": "E_\\theta(x) : \\mathbb{R}^d \\to \\mathbb{R}",
        "explanation": "THE CORE. Energy function E_θ(x) maps input x to scalar energy. Low energy = plausible data, high energy = implausible. Learned via contrastive divergence or score matching.",
        "variables": {
          "E_θ(x)": "Energy of input x",
          "θ": "Model parameters (neural network weights)",
          "x": "Input data (image, vector, etc.)"
        }
      },
      {
        "name": "Gibbs Distribution (Boltzmann Distribution)",
        "latex": "p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta} = \\frac{e^{-E_\\theta(x)}}{\\int e^{-E_\\theta(x')} dx'}",
        "explanation": "Probability defined via energy. Exponential ensures p(x) > 0. Z_θ is partition function (intractable for high-dim x). Lower energy → higher probability.",
        "variables": {
          "p_θ(x)": "Probability density of x",
          "Z_θ": "Partition function (normalizing constant)",
          "e^(-E)": "Unnormalized probability (Boltzmann factor)"
        }
      },
      {
        "name": "Contrastive Divergence Loss",
        "latex": "\\mathcal{L}_{CD} = \\mathbb{E}_{x \\sim p_{data}}[E_\\theta(x)] - \\mathbb{E}_{\\tilde{x} \\sim p_{\\theta}}[E_\\theta(\\tilde{x})]",
        "explanation": "Push down energy on real data, push up on model samples x̃. Approximates maximum likelihood. p_θ samples via MCMC (Langevin dynamics).",
        "variables": {
          "p_data": "True data distribution",
          "p_θ": "Model distribution (samples via MCMC)",
          "x̃": "Negative samples (generated by model)"
        }
      },
      {
        "name": "Langevin Dynamics Sampling",
        "latex": "x_{t+1} = x_t - \\frac{\\epsilon}{2} \\nabla_x E_\\theta(x_t) + \\sqrt{\\epsilon} \\, z_t, \\quad z_t \\sim \\mathcal{N}(0, I)",
        "explanation": "MCMC sampling from p_θ. Gradient ∇E pushes toward low energy, noise z explores. ε is step size. Converges to Gibbs distribution as t → ∞.",
        "variables": {
          "x_t": "Sample at step t",
          "ε": "Step size (typically 0.01)",
          "∇_x E": "Energy gradient w.r.t. input",
          "z_t": "Gaussian noise for exploration"
        }
      },
      {
        "name": "Score Matching Loss",
        "latex": "\\mathcal{L}_{SM} = \\mathbb{E}_{x \\sim p_{data}} \\left[\\frac{1}{2} \\|\\nabla_x E_\\theta(x)\\|^2 + \\text{tr}(\\nabla_x^2 E_\\theta(x))\\right]",
        "explanation": "Train E without sampling! Match score ∇log p_data. Avoids MCMC but requires Hessian (expensive). Equivalent to denoising score matching.",
        "variables": {
          "∇_x E": "Energy gradient (score function)",
          "tr(∇²E)": "Trace of Hessian (Laplacian)",
          "||·||²": "L2 norm squared"
        }
      },
      {
        "name": "Noise Contrastive Estimation (NCE)",
        "latex": "\\mathcal{L}_{NCE} = -\\mathbb{E}_{x \\sim p_{data}}[\\log \\sigma(-E_\\theta(x))] - \\mathbb{E}_{x' \\sim p_{noise}}[\\log \\sigma(E_\\theta(x'))]",
        "explanation": "Binary classification: real data vs noise. σ is sigmoid. Avoids partition function Z. Noise p_noise is tractable (e.g., Gaussian). Used in NLP (word2vec).",
        "variables": {
          "σ": "Sigmoid function",
          "p_noise": "Simple noise distribution",
          "E(x)": "Energy (logit for classification)"
        }
      }
    ],
    "architectures": [
      {
        "name": "ConvNet EBM",
        "description": "CNN backbone for image energy: Conv layers → Global pooling → FC → scalar energy"
      },
      {
        "name": "Transformer EBM",
        "description": "Self-attention for sequential/graph data, outputs energy per token or global energy"
      }
    ]
  },
  "code": {
    "framework": "PyTorch",
    "implementation": "class EnergyBasedModel(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=128):\n        super().__init__()\n        # Energy network: input -> hidden -> scalar energy\n        self.energy_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)  # Output: scalar energy E(x)\n        )\n    \n    def energy(self, x):\n        \"\"\"Compute energy E_θ(x). Low = plausible, high = implausible.\"\"\"\n        return self.energy_net(x).squeeze(-1)  # Shape: (batch,)\n    \n    def sample_langevin(self, x_init, n_steps=60, step_size=0.01, noise_scale=0.005):\n        \"\"\"Sample from p_θ using Langevin dynamics MCMC.\"\"\"\n        x = x_init.clone().detach().requires_grad_(True)\n        \n        for _ in range(n_steps):\n            # Compute energy gradient\n            energy = self.energy(x).sum()\n            grad = torch.autograd.grad(energy, x, create_graph=False)[0]\n            \n            # Langevin update: x -= ε/2 * ∇E + sqrt(ε) * noise\n            x = x - step_size/2 * grad + noise_scale * torch.randn_like(x)\n            x = x.detach().requires_grad_(True)\n        \n        return x.detach()\n    \n    def contrastive_divergence_loss(self, x_pos):\n        \"\"\"Contrastive divergence: E(data) - E(model_samples).\"\"\"\n        # Positive phase: energy on real data\n        energy_pos = self.energy(x_pos).mean()\n        \n        # Negative phase: sample from model via Langevin\n        x_neg = torch.randn_like(x_pos)  # Random init\n        x_neg = self.sample_langevin(x_neg, n_steps=60)\n        energy_neg = self.energy(x_neg).mean()\n        \n        # CD loss: push down E(data), push up E(samples)\n        loss = energy_pos - energy_neg\n        return loss, x_neg\n\n# Training loop\nmodel = EnergyBasedModel(input_dim=784, hidden_dim=128)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nfor epoch in range(100):\n    for x_batch in data_loader:  # x_batch: real MNIST images\n        x_batch = x_batch.view(x_batch.size(0), -1)  # Flatten to 784\n        \n        optimizer.zero_grad()\n        loss, x_neg = model.contrastive_divergence_loss(x_batch)\n        loss.backward()\n        optimizer.step()\n        \n        # Optional: visualize negative samples x_neg to see what model generates\n    \n    print(f'Epoch {epoch}, CD Loss: {loss.item():.4f}')\n\n# Generation: start from noise, run long Langevin chain\nx_gen = torch.randn(16, 784)\nx_gen = model.sample_langevin(x_gen, n_steps=1000, step_size=0.01)\n# x_gen now contains generated samples from p_θ",
    "keyComponents": [
      "Energy network E_θ(x) → scalar",
      "Langevin MCMC for sampling",
      "Contrastive divergence training",
      "Gradient-based energy minimization"
    ]
  },
  "useCases": [
    {
      "title": "Image Generation",
      "description": "Generate images by sampling from low-energy regions via Langevin dynamics. Used in IGEBM (Implicit Generation and Modeling) for high-quality CIFAR-10/ImageNet generation.",
      "example": "IGEBM: 43.5 FID on CIFAR-10 (2019)"
    },
    {
      "title": "Out-of-Distribution Detection",
      "description": "Assign high energy to OOD data. Energy threshold distinguishes in-distribution (low E) from OOD (high E). More reliable than softmax confidence.",
      "example": "ImageNet vs SVHN: 95%+ OOD detection AUROC"
    },
    {
      "title": "Compositional Reasoning",
      "description": "Combine multiple EBMs: E_total = E_1 + E_2. Enables AND/OR logic (e.g., 'red car' = E_red + E_car). Product of Experts (PoE) for multi-modal learning.",
      "example": "Compose 'digit 7' + 'red color' EBMs for conditional generation"
    },
    {
      "title": "Adversarial Robustness",
      "description": "Adversarial training with energy-based purification. Project adversarial examples to low-energy manifold via gradient descent on E(x).",
      "example": "Energy-based purification: +10% robust accuracy on CIFAR-10"
    }
  ],
  "benchmarks": {
    "CIFAR-10 Generation (FID)": "43.5 (IGEBM, 2019)",
    "ImageNet 32x32 (FID)": "38.2 (JEM, 2020)",
    "OOD Detection (AUROC)": "95.4% (ImageNet vs SVHN)",
    "Training Time": "3-5 days on 4 V100 GPUs (CIFAR-10)"
  },
  "trainingTips": [
    {
      "tip": "Initialize with short Langevin chains (20-60 steps) during training, longer chains (200-1000) for generation",
      "reason": "Short chains = fast training but biased samples. Long chains = unbiased but slow."
    },
    {
      "tip": "Use replay buffer to store past negative samples, reinitialize 5% from buffer",
      "reason": "Reduces MCMC burn-in time. Buffer stores 'hard negatives' near data manifold."
    },
    {
      "tip": "Clip gradients to [-0.03, 0.03] and use spectral normalization on energy network",
      "reason": "Prevents energy explosion. EBMs prone to mode collapse if gradients unbounded."
    },
    {
      "tip": "Add noise to data during training: x + σ·N(0,I) with σ decaying over epochs",
      "reason": "Smooths energy landscape, aids MCMC mixing. Start σ=0.05, decay to 0.01."
    }
  ],
  "comparisons": ["vae", "gan", "diffusion", "flow-based"],
  "resources": [
    {
      "type": "paper",
      "title": "A Tutorial on Energy-Based Learning",
      "url": "http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf",
      "description": "LeCun's foundational tutorial (2006)"
    },
    {
      "type": "paper",
      "title": "Implicit Generation and Modeling with Energy-Based Models (IGEBM)",
      "url": "https://arxiv.org/abs/1903.08689",
      "description": "Modern EBM for image generation (NeurIPS 2019)"
    },
    {
      "type": "paper",
      "title": "Your Classifier is Secretly an Energy Based Model (JEM)",
      "url": "https://arxiv.org/abs/1912.03263",
      "description": "Joint energy-based classification + generation (ICLR 2020)"
    },
    {
      "type": "blog",
      "title": "Energy-Based Models (OpenAI Blog)",
      "url": "https://openai.com/research/energy-based-models",
      "description": "Practical intro to EBMs"
    },
    {
      "type": "code",
      "title": "PyTorch EBM Tutorial",
      "url": "https://github.com/yilundu/improved_contrastive_divergence",
      "description": "Implementation of IGEBM and variants"
    }
  ],
  "tags": ["ebm", "energy-based", "probabilistic", "generative", "mcmc", "langevin", "2006"],
  "difficulty": "Advanced",
  "computationalRequirements": {
    "minimumVRAM": "8 GB",
    "recommendedVRAM": "16 GB (for ImageNet)",
    "trainingTime": {
      "cifar10": "3-5 days on 4x V100",
      "imagenet32": "1-2 weeks on 8x V100"
    },
    "typicalBatchSize": 128,
    "notes": "MCMC sampling is slow (60+ steps per batch). Consider persistent chains or replay buffer."
  }
}
