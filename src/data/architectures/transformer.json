{
  "id": "transformer",
  "name": "Transformer (Vanilla)",
  "category": "transformer",
  "subcategory": "core",
  "year": 2017,
  "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin"],
  "paper": "Attention Is All You Need",
  "paperUrl": "https://arxiv.org/abs/1706.03762",
  "description": "The Transformer is a revolutionary architecture that replaced recurrence with self-attention mechanisms. It processes entire sequences in parallel rather than sequentially, enabling much faster training and better long-range dependency modeling.",
  "plainEnglish": "Think of reading a book - you can jump to any page and understand context by looking at surrounding pages. The Transformer does the same with text or data: instead of reading word-by-word like RNNs, it can 'attend' to any part of the input simultaneously, understanding relationships between all words at once.",
  "keyInnovation": "Multi-head self-attention mechanism that processes sequences in parallel and captures long-range dependencies without recurrence or convolution.",
  "architecture": {
    "layers": [
      {"type": "embedding", "name": "Input Embedding", "params": {"vocab_size": 30000, "d_model": 512}},
      {"type": "positional_encoding", "name": "Positional Encoding", "params": {"d_model": 512, "max_len": 5000}},
      {"type": "encoder", "name": "Encoder Stack", "params": {"num_layers": 6, "d_model": 512, "num_heads": 8, "d_ff": 2048}},
      {"type": "decoder", "name": "Decoder Stack", "params": {"num_layers": 6, "d_model": 512, "num_heads": 8, "d_ff": 2048}},
      {"type": "linear", "name": "Output Projection", "params": {"d_model": 512, "vocab_size": 30000}}
    ],
    "parameters": 65000000,
    "depth": 12,
    "inputShape": ["sequence_length"],
    "outputShape": ["sequence_length", "vocab_size"]
  },
  "mathematics": {
    "equations": [
      {
        "name": "Scaled Dot-Product Attention",
        "latex": "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",
        "description": "Computes attention weights by comparing queries with keys, then applies to values"
      },
      {
        "name": "Multi-Head Attention",
        "latex": "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O",
        "description": "Multiple attention heads learn different representation subspaces"
      },
      {
        "name": "Positional Encoding",
        "latex": "PE_{(pos,2i)} = \\sin(pos/10000^{2i/d_{model}}), \\quad PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d_{model}})",
        "description": "Injects position information using sinusoidal functions"
      },
      {
        "name": "Feed-Forward Network",
        "latex": "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2",
        "description": "Two-layer MLP applied to each position independently"
      }
    ],
    "forwardPass": [
      "Embed input tokens and add positional encoding",
      "Pass through N encoder layers (self-attention + FFN)",
      "Embed output tokens (shifted right) and add positional encoding",
      "Pass through N decoder layers (masked self-attention + cross-attention + FFN)",
      "Project to vocabulary size and apply softmax"
    ],
    "lossFunction": "Cross-Entropy with Label Smoothing"
  },
  "code": {
    "pytorch": "import torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        # Linear projections\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q, K, V: (batch_size, num_heads, seq_len, d_k)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attention = torch.softmax(scores, dim=-1)\n        output = torch.matmul(attention, V)\n        return output, attention\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # Linear projections and reshape for multi-head attention\n        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Apply attention\n        x, attention = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate heads and apply output projection\n        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        output = self.W_o(x)\n        \n        return output, attention\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        \n        # Create positional encoding matrix\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        # Self-attention with residual connection and layer norm\n        attn_output, _ = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed-forward with residual connection and layer norm\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        \n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n        # Masked self-attention\n        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Cross-attention with encoder output\n        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        # Feed-forward\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        \n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, dropout=0.1, max_len=5000):\n        super().__init__()\n        \n        # Embeddings\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_len)\n        \n        # Encoder and Decoder stacks\n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_encoder_layers)\n        ])\n        \n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_decoder_layers)\n        ])\n        \n        # Output projection\n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(d_model)\n        \n    def encode(self, src, src_mask=None):\n        x = self.positional_encoding(self.src_embedding(src) * self.scale)\n        x = self.dropout(x)\n        \n        for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n        \n        return x\n    \n    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n        x = self.positional_encoding(self.tgt_embedding(tgt) * self.scale)\n        x = self.dropout(x)\n        \n        for layer in self.decoder_layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        \n        return x\n    \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        encoder_output = self.encode(src, src_mask)\n        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n        output = self.fc_out(decoder_output)\n        return output\n\n# Usage\nmodel = Transformer(src_vocab_size=30000, tgt_vocab_size=30000)\nprint(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
  },
  "useCases": [
    {
      "title": "Machine Translation",
      "description": "Original use case - translating text between languages",
      "examples": ["English to German", "Multilingual translation", "Document translation"],
      "industry": "Natural Language Processing"
    },
    {
      "title": "Text Generation",
      "description": "Foundation for modern language models like GPT",
      "examples": ["Story generation", "Code completion", "Chatbots"],
      "industry": "AI/ML"
    },
    {
      "title": "Sequence-to-Sequence Tasks",
      "description": "Any task involving mapping input sequences to output sequences",
      "examples": ["Summarization", "Question answering", "Speech recognition"],
      "industry": "NLP"
    }
  ],
  "benchmarks": {
    "datasets": [
      {"name": "WMT14 EN-DE", "accuracy": 28.4, "otherMetrics": {"BLEU": 28.4}},
      {"name": "WMT14 EN-FR", "accuracy": 41.8, "otherMetrics": {"BLEU": 41.8}}
    ],
    "performance": {
      "speed": "3.5x faster training than LSTM on similar hardware",
      "memory": "Variable depending on sequence length",
      "accuracy": "State-of-the-art on translation tasks (2017)"
    }
  },
  "trainingTips": {
    "hyperparameters": {
      "d_model": 512,
      "num_heads": 8,
      "num_layers": 6,
      "d_ff": 2048,
      "dropout": 0.1,
      "learning_rate": "Warmup then decay",
      "batch_size": "25000 tokens per batch",
      "optimizer": "Adam (β1=0.9, β2=0.98, ε=1e-9)"
    },
    "commonIssues": [
      {
        "problem": "Training instability early on",
        "solution": "Use learning rate warmup: start at 0 and increase to peak over first 4000 steps"
      },
      {
        "problem": "Poor performance on long sequences",
        "solution": "Use relative positional encoding or increase max_len in positional encoding"
      },
      {
        "problem": "Out of memory",
        "solution": "Reduce batch size or sequence length; use gradient checkpointing"
      }
    ],
    "dataRequirements": "Millions of parallel sentence pairs for translation. Can work with less data using transfer learning.",
    "trainingTime": "12 hours on 8 P100 GPUs for WMT14 EN-DE (100K steps)"
  },
  "comparisons": ["lstm", "gru", "seq2seq", "bert", "gpt2"],
  "resources": [
    {
      "type": "paper",
      "title": "Attention Is All You Need",
      "url": "https://arxiv.org/abs/1706.03762",
      "author": "Vaswani et al."
    },
    {
      "type": "blog",
      "title": "The Illustrated Transformer",
      "url": "https://jalammar.github.io/illustrated-transformer/"
    },
    {
      "type": "implementation",
      "title": "Annotated Transformer",
      "url": "http://nlp.seas.harvard.edu/annotated-transformer/"
    }
  ],
  "tags": ["nlp", "attention", "self-attention", "seq2seq", "parallel-processing"],
  "difficulty": "Advanced",
  "computationalRequirements": {
    "minGPU": "NVIDIA GTX 1080 Ti (11GB)",
    "minRAM": "16GB",
    "recommendedGPU": "NVIDIA A100 or V100",
    "recommendedRAM": "64GB"
  }
}
